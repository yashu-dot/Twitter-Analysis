{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 840 Mini-Project: Twitter Sentiment Analysis\n",
    "\n",
    "### Due date: Sunday, April 30, 2023 (before midnight) \n",
    "\n",
    "The objective of this project is to train a classifier for predicting the sentiment of a tweet. You are only allowed to use the standard python library (including python regular expression library) along with the matplotlib, pandas, and numpy libraries to implement the code. No other library functions are allowed (except for tqdm to monitor progress of your program execution). You must use the Jupyter notebook template provided here to write your program. Rename the notebook as **project_yourlastname.ipynb** and submit it to D2L. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# List of libraries permitted to be used for the project\n",
    "##########################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "##########################################################################################\n",
    "# The following two functions can be used to split the data and evaluate your classifier.\n",
    "# No other functions in scikit-learn library are permitted.\n",
    "##########################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "numTweets = 50000        # Number of tweets to be preprocessed (can be reduced if it slows down your machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0:** Download the `twitter.csv` file from D2L and load it into the Jupyter notebook. Note that the original data file contains 1 million tweets, each has been classified as having positive or negative sentiment. If you're running the code on a smaller machine, you can limit your analysis to 50,000 or 100,000 tweets by setting the appropriate numTweets parameter below. However, the smaller the number of tweets, the more likely your classifier may overfit the training data and achieves slightly lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post birthday and feeling great</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Has 2 go 2 sunday school w/o 2 of my favorite ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is this incessantly bright skin-burning s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@downesy That link didn't work, squire    What...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@musicaddicted13 sowie about your mouth cai</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I'm the operator with my pocket calculator ......</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@fishhhface Yeah, keyword is 'cute'. In the ot...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Goooooooodnaaaaayt sa lahat  http://plurk.com/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Doing coursework  .. Its sunny and I want to b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wish there was still the playoffs on tv  ARGH!...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bed time.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>late-ish to school xD    blahh    mood; sunny</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OMG, Kevin is here at Pizza Club!!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>second Blink show added in Irvine!! No Weezer ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>THIS RADIO STREAM IS HITTIN BDAY GIRLS GETTIN ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>You yes you lookin at your phone. BOO! scared ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>net capped @ home,unable 2 do notes  so sittin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>day off</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@mollyroloff tell zach &amp;amp; jer I said happy ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ready to celebrate Kanani &amp;amp; Gabe's wedding...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment\n",
       "0                    post birthday and feeling great   positive\n",
       "1   Has 2 go 2 sunday school w/o 2 of my favorite ...  negative\n",
       "2   What is this incessantly bright skin-burning s...  negative\n",
       "3   @downesy That link didn't work, squire    What...  negative\n",
       "4        @musicaddicted13 sowie about your mouth cai   negative\n",
       "5   I'm the operator with my pocket calculator ......  positive\n",
       "6   @fishhhface Yeah, keyword is 'cute'. In the ot...  positive\n",
       "7   Goooooooodnaaaaayt sa lahat  http://plurk.com/...  positive\n",
       "8   Doing coursework  .. Its sunny and I want to b...  negative\n",
       "9   Wish there was still the playoffs on tv  ARGH!...  negative\n",
       "10                                         Bed time.   positive\n",
       "11     late-ish to school xD    blahh    mood; sunny   positive\n",
       "12               OMG, Kevin is here at Pizza Club!!!   positive\n",
       "13  second Blink show added in Irvine!! No Weezer ...  negative\n",
       "14  THIS RADIO STREAM IS HITTIN BDAY GIRLS GETTIN ...  positive\n",
       "15  You yes you lookin at your phone. BOO! scared ...  positive\n",
       "16  net capped @ home,unable 2 do notes  so sittin...  negative\n",
       "17                                           day off   positive\n",
       "18  @mollyroloff tell zach &amp; jer I said happy ...  positive\n",
       "19  ready to celebrate Kanani &amp; Gabe's wedding...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/yashashvini/Desktop/MSU/CSE840/project/twitter.csv',nrows=numTweets)\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. DATA EXPLORATION AND PREPROCESSING\n",
    "\n",
    "In this step, you need to write the code to preprocess the twitter data and perform some exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Write a function named `preprocess` that takes an input string and preprocesses it in the following way. Make sure you follow the order of the preprocessing steps listed below to avoid making any mistakes:\n",
    "\n",
    "1. Convert all the characters in the tweet message into lower case.\n",
    "\n",
    "2. Convert all occurrences of the following markup strings in the tweet message: \n",
    "`\\&quot;` `\\&amp;` `\\&gt;` and `\\&lt` into white space characters.\n",
    "\n",
    "3. Replace every occurrence of the following special characters in the tweet message: \n",
    "<pre>tab comma semicolon doublequote(\"\") ! ? + = * | (  )  [  ] {  }</pre>\n",
    "with a white space character. However, **do not remove** the following special characters:\n",
    "<pre>period(.)  colon(:)  percent(%)  singlequote(')  dash(-)  slashes(\\ or /)  underscore(_)  alias(@)  hashtag(#)</pre>\n",
    "from the tweet message.\n",
    "\n",
    "4. Remove all the periods except for those located between 2 non-period characters. You should also replace any occurrence of 2 or more consecutive period characters with a white space character. For example:\n",
    "<pre>Before preprocessing: Hey...there! My GPA is only 3.5..according to http://student.msu.edu.</pre>\n",
    "<pre> After preprocessing: hey there my gpa is only 3.5 according to http://student.msu.edu</pre>\n",
    "\n",
    "5. Remove all the extra white space characters so that every word token is separated by a single white space character. For example:\n",
    "<pre>Before preprocessing: Good        morning</pre>\n",
    "<pre> After preprocessing: good morning</pre>\n",
    "\n",
    "For this step, you should use the python regular expression library (re) to preprocess the text. For more information and examples of how to use the library, you may refer to the documentation and examples given here\n",
    "- https://docs.python.org/3/library/re.html\n",
    "- https://pynative.com/python-regex-replace-re-sub/\n",
    "\n",
    "The function should return a string object that represents the preprocessed input text. You are prohibited from using any other text processing libraries (such as nltk and scikit-learn) to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    \"\"\"\"\n",
    "        Function to preprocess an input string (e.g., a tweet message)\n",
    "            Input: a string\n",
    "            Output: a string\n",
    "            \n",
    "        Make sure you follow the order of the preprocessing steps stated above to avoid generating \n",
    "        incorrect answer.\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"&quot;|&amp;|&gt;|&lt;\", \" \", sentence)\n",
    "    sentence = re.sub(r'[&<>\"\\\"]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\.{2,}', ' ', sentence)\n",
    "    sentence = re.sub(r'([a-zA-Z])\\.(\\s|$)', r'\\1\\2', sentence)\n",
    "    sentence = re.sub(r\"\\.(?=[^A-Za-z0-9])\", \"\", sentence)\n",
    "    sentence = re.sub(r'\\t|,|;|\"|!|\\?|\\+|=|\\*|\\||\\(|\\)|\\[|\\]|\\{|\\}', \" \", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for testing correctness of the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Hey...there. My GPA is only 3.5..according to http://student.msu.edu.\n",
      " After: hey there my gpa is only 3.5 according to http://student.msu.edu\n"
     ]
    }
   ],
   "source": [
    "str = \"Hey...there. My GPA is only 3.5..according to http://student.msu.edu.\"\n",
    "print('Before:', str)\n",
    "print(' After:',preprocess(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to show the result of preprocessing on a subset of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Wish there was still the playoffs on tv  ARGH!!! Nothing good is going on these dayz...other than &quot;I'm a celebrity, get me outta here&quot; :/\n",
      "After:  wish there was still the playoffs on tv argh nothing good is going on these dayz other than i'm a celebrity get me outta here :/ \n",
      "\n",
      "Before: You yes you lookin at your phone. BOO! scared ya.  call me!  &lt;Superman&gt;\n",
      "After:  you yes you lookin at your phone boo scared ya call me superman \n",
      "\n",
      "Before: Done with KEN-YA DANCE. It was chaotic but fun. All pics will be up on MySpace &amp; Facebook -- that is if you're on my friends list. \n",
      "After:  done with ken-ya dance it was chaotic but fun all pics will be up on myspace facebook -- that is if you're on my friends list \n",
      "\n",
      "Before: @faceman101 *cough* on O'Connell street in the little kiosk *cough* \n",
      "After:  @faceman101 cough on o'connell street in the little kiosk cough \n",
      "\n",
      "Before: I'm outta here 4 bit. Too pooped 2 think...not that I do it very well anyway...(just kidding).  Have a great night all u tweeties. \n",
      "After:  i'm outta here 4 bit too pooped 2 think not that i do it very well anyway just kidding have a great night all u tweeties \n",
      "\n",
      "Before: I am such a procrastinator...I should really be packing for my trip tomorrow p.s. dreading waking up before noon \n",
      "After:  i am such a procrastinator i should really be packing for my trip tomorrow p.s dreading waking up before noon \n",
      "\n",
      "Before: @sarahreesbrenna  yes only called 'drumsticks&quot; &amp; have eaten 2 this week...maybe that is just the brand...but cornetto isn't a word here \n",
      "After:  @sarahreesbrenna yes only called 'drumsticks have eaten 2 this week maybe that is just the brand but cornetto isn't a word here \n",
      "\n",
      "Before: Flyer to promote #Chuck Me Mondays: http://images.baylink.com/chuck/onesheet1.pdf - now with 100% less fanboi language! \n",
      "After:  flyer to promote #chuck me mondays: http://images.baylink.com/chuck/onesheet1.pdf - now with 100% less fanboi language \n",
      "\n",
      "Before: OMG it's the morning  GOOOODMORNINGGGGGGGGGG!!! @mcflyismydrug_x - i have to tell you I'm having toast in my house today 8-)\n",
      "After:  omg it's the morning goooodmorningggggggggg @mcflyismydrug_x - i have to tell you i'm having toast in my house today 8- \n",
      "\n",
      "Before: Kahkahaya haz?r olun.  --&gt; Stavros Flatly - Greek Irish Dancers - Britains Got Talent 2009... http://ff.im/3oVut\n",
      "After:  kahkahaya haz r olun -- stavros flatly - greek irish dancers - britains got talent 2009 http://ff.im/3ovut \n",
      "\n",
      "Before: @tombrokeoff I was incredibly irritated. Very articulate + red hair = highly flammable \n",
      "After:  @tombrokeoff i was incredibly irritated very articulate red hair highly flammable \n",
      "\n",
      "Before: Rainy day  Maxin' out and watching &quot;Yes Man.&quot;\n",
      "After:  rainy day maxin' out and watching yes man \n",
      "\n",
      "Before: @DavidArchie hey DAViD! so, how was your flight.? hope you enjoyed your stay here in the philippines.. we'll miss you   come back soon \n",
      "After:  @davidarchie hey david so how was your flight hope you enjoyed your stay here in the philippines we'll miss you come back soon \n",
      "\n",
      "Before: @atbandre that's so cool that you have a black bunny. I hope you liked the picture of our mr.pebbles he is a black too \n",
      "After:  @atbandre that's so cool that you have a black bunny i hope you liked the picture of our mr.pebbles he is a black too \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in iter([9,15,45,73,81,89,111,113,132,135,138,144,189,190]):\n",
    "    print('Before:', data['text'][i])\n",
    "    print('After: ', preprocess(data['text'][i]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Apply the preprocessing function to the twitter data column of the input dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post birthday and feeling great</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has 2 go 2 sunday school w/o 2 of my favorite ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is this incessantly bright skin-burning s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@downesy that link didn't work squire what wer...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@musicaddicted13 sowie about your mouth cai</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i'm the operator with my pocket calculator ham...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@fishhhface yeah keyword is 'cute' in the othe...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>goooooooodnaaaaayt sa lahat http://plurk.com/p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doing coursework its sunny and i want to be ou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wish there was still the playoffs on tv argh n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bed time</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>late-ish to school xd blahh mood sunny</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>omg kevin is here at pizza club</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>second blink show added in irvine no weezer th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>this radio stream is hittin bday girls gettin ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>you yes you lookin at your phone boo scared ya...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>net capped @ home unable 2 do notes so sitting...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>day off</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@mollyroloff tell zach jer i said happy birthd...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ready to celebrate kanani gabe's wedding then ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment\n",
       "0                     post birthday and feeling great  positive\n",
       "1   has 2 go 2 sunday school w/o 2 of my favorite ...  negative\n",
       "2   what is this incessantly bright skin-burning s...  negative\n",
       "3   @downesy that link didn't work squire what wer...  negative\n",
       "4         @musicaddicted13 sowie about your mouth cai  negative\n",
       "5   i'm the operator with my pocket calculator ham...  positive\n",
       "6   @fishhhface yeah keyword is 'cute' in the othe...  positive\n",
       "7   goooooooodnaaaaayt sa lahat http://plurk.com/p...  positive\n",
       "8   doing coursework its sunny and i want to be ou...  negative\n",
       "9   wish there was still the playoffs on tv argh n...  negative\n",
       "10                                           bed time  positive\n",
       "11             late-ish to school xd blahh mood sunny  positive\n",
       "12                    omg kevin is here at pizza club  positive\n",
       "13  second blink show added in irvine no weezer th...  negative\n",
       "14  this radio stream is hittin bday girls gettin ...  positive\n",
       "15  you yes you lookin at your phone boo scared ya...  positive\n",
       "16  net capped @ home unable 2 do notes so sitting...  negative\n",
       "17                                            day off  positive\n",
       "18  @mollyroloff tell zach jer i said happy birthd...  positive\n",
       "19  ready to celebrate kanani gabe's wedding then ...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(preprocess)\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Draw a bar chart to display the class distribution, i.e., proportion of tweets with positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.sentiment.value_counts()[:20].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** In this step, you will test the following hypotheses:\n",
    "- Does a tweet that contains at least one hashtag will more likely have positive or negative sentiment? \n",
    "- Does a tweet that contains at least one mention will more likely have positive or negative sentiment? \n",
    "- Does a tweet that contains at least one URL will more likely have positive or negative sentiment? \n",
    "\n",
    "To do this, you need to write a function named `checkTweet` that checks whether a tweet contains a particular word token. If so, the function should return the value 1. Otherwise, it returns the value 0. You can use the python regular expression library function re.search() to do this. You may refer to the examples given in https://developers.google.com/edu/python/regular-expressions.\n",
    "\n",
    "You will apply the function to determine whether a tweet message contains a `hashtag`, a `mention`, or a `URL`. Note that each hashtag begins with the character `#`, each mention begins with the character `@`, while each URL begins with the prefix `http`. You need to be careful when specifying the regular expression pattern to search for in a string since the special characters such as `#` or `@` do not always appear at the beginning of the word token. For example, email addresses like `cse840@msu.edu` also contains an `@` character but should not be counted as a tweet mention.\n",
    "\n",
    "Apply the function to the twitter text column of the dataframe object and store the result as new columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>mention</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>just finishing packing then it's back to the r...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>@sarahreesbrenna yes only called 'drumsticks h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>nighty night world teaching the 3's and 4's to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>flyer to promote #chuck me mondays: http://ima...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>@pascalgrob i want one</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>clinical education back in the evening later o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>@simikn awww i miss u like crazy alreadyyy x</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>@jessemccartney i miss you</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>i think i have an eye infection</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>@sicilyyoder sounds good but you are right thi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text sentiment  hashtag  \\\n",
       "110  just finishing packing then it's back to the r...  negative        0   \n",
       "111  @sarahreesbrenna yes only called 'drumsticks h...  positive        0   \n",
       "112  nighty night world teaching the 3's and 4's to...  positive        0   \n",
       "113  flyer to promote #chuck me mondays: http://ima...  positive        1   \n",
       "114                             @pascalgrob i want one  negative        0   \n",
       "115  clinical education back in the evening later o...  negative        0   \n",
       "116       @simikn awww i miss u like crazy alreadyyy x  negative        0   \n",
       "117                         @jessemccartney i miss you  negative        0   \n",
       "118                    i think i have an eye infection  negative        0   \n",
       "119  @sicilyyoder sounds good but you are right thi...  positive        0   \n",
       "\n",
       "     mention  url  \n",
       "110        0    0  \n",
       "111        1    0  \n",
       "112        0    0  \n",
       "113        0    1  \n",
       "114        1    0  \n",
       "115        0    0  \n",
       "116        1    0  \n",
       "117        1    0  \n",
       "118        0    0  \n",
       "119        1    0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checkTweet(text, pattern):\n",
    "    \"\"\"\n",
    "        Function to check whether a tweet message contains a pattern. Use python regular expression\n",
    "        library to implement this step. \n",
    "        \n",
    "        Input:\n",
    "            text: tweet message to be processed.\n",
    "            pattern: pattern to search for within the text.\n",
    "            \n",
    "        Output:\n",
    "            1 if the text contains the pattern; otherwise return the integer value 0.\n",
    "        \n",
    "    \"\"\"\n",
    "    if pattern == '^#' :\n",
    "        pattern = '\\B#\\w+'\n",
    "    if pattern == '^@':\n",
    "        pattern = '\\B@\\w+'\n",
    "    if pattern == '^http':\n",
    "        pattern = 'https?://\\S+'\n",
    "    \n",
    "    match = re.search(pattern, text)\n",
    "\n",
    "    return 1 if match else 0\n",
    "\n",
    "    \n",
    "    \n",
    "######################################################################\n",
    "# Show result of applying checkTweet to the twitter data\n",
    "######################################################################\n",
    "data['hashtag'] = data['text'].apply(lambda x: checkTweet(x, '^#'))\n",
    "data['mention'] = data['text'].apply(lambda x: checkTweet(x, '^@'))\n",
    "data['url'] = data['text'].apply(lambda x: checkTweet(x, '^http'))\n",
    "data[110:120]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Apply the pandas `crosstab` function to determine the statistics of tweets having `hashtags`, `mentions`, and `URLs`.  For more information about how to use the function, you should refer to https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html.\n",
    "\n",
    "**Note:** The counts below are for illustrative purposes only. The actual number depends on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment  negative  positive\n",
      "hashtag                      \n",
      "0             24430     24474\n",
      "1               468       628 \n",
      "\n",
      "sentiment  negative  positive\n",
      "mention                      \n",
      "0             15582     11220\n",
      "1              9316     13882 \n",
      "\n",
      "sentiment  negative  positive\n",
      "url                          \n",
      "0             24193     23668\n",
      "1               705      1434\n"
     ]
    }
   ],
   "source": [
    "print(pd.crosstab(data.hashtag,data.sentiment), '\\n')\n",
    "print(pd.crosstab(data.mention,data.sentiment), '\\n')\n",
    "print(pd.crosstab(data.url,data.sentiment))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Based on the statistics above, what can you conclude about tweets that contain `hashtags`, `mentions`, and `URLs`. Will they more likely correspond to positive or negative sentiment tweets?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "On comparing \n",
    "1. positive sentences have high number of hashtags, mentions, urls\n",
    "\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. FEATURE EXTRACTION\n",
    "\n",
    "For this step, you need to implement a python class named `TwitterTokenizer` to create a feature representation of a given set of tweets as shown in the diagram below. \n",
    "\n",
    "<img src=\"preprocessing.png\" width=800>\n",
    "<pre>Figure 1: The transformation of a set of tweets into its corresponding matrix of feature vectors.</pre>\n",
    "\n",
    "Each element of the matrix contains the frequency of a word token in the given tweet. For example, the last tweet contains the word token *tweet* appearing twice and the word token *last* appearing 3 times in the tweet. The size of the feature matrix is # tweets x # words. \n",
    "\n",
    "The **document frequency (df)** of a word token corresponds to the number of tweets containing the word token. For example, the document frequency for the word token *tweet* is 4 while the document frequency for the word token *last* is equal to 1. As the number of words in a large corpus of tweets can be massive, you should only keep track of word tokens whose document frequencies are between some min_freq and max_freq thresholds. For example, if min_freq = 1 and max_freq = 3, then the feature matrix will contain only 6 word tokens - *this*, *first*, *here*, *second*, *third*, and *last*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Write an implementation of a python class named `TwitterTokenizer` that contains the following functions:\n",
    "- `init`: This is a constructor function that is invoked when the class is instantiated. You can specify the minimum and maximum document frequency of the word tokens as parameters of the function. \n",
    "- `fit`: The function takes as input a numpy array containing the tweet messages to be processed. The function will then map each unique word in the tweet messages to a word ID. You should use a dictionary object to store the mapping of each word to its corresponding word ID.\n",
    "    <pre>dictionary: key = word,  value = wordID</pre>\n",
    "For example, in the diagram shown in Figure 1, the dictionary will map the word \"this\" to ID=0, \"is\" to ID=1, and so on. Furthermore, the function should only store word tokens whose document frequencies are greater than or equal to the min_freq threshold and are less than or equal to the max_freq threshold.\n",
    "\n",
    "- `transform`: This function takes a numpy array of tweets as input and returns the feature representation of the tweets using the dictionary created by the fit() function. \n",
    "- `getDictionary`: This function returns a dictionary that contains the mapping of each word to its word ID.\n",
    "- `displayVector`: This function takes a feature vector as its input and returns the list of words associated with the vector along with their corresponding word frequencies. The purpose of this function is to check the correctness of your TwitterTokenizer class implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterTokenizer(object):\n",
    "    def __init__(self, min_df = 1, max_df = 1000):\n",
    "        \"\"\"\n",
    "         A constructor function to initialize the state of the object. The function stores a dictionary variable\n",
    "         named vocabulary that contains the mapping of each word to its corresponding word ID. The function\n",
    "         takes as input a pair of threshold parameters that specify the minimum and maximum frequency of a word\n",
    "         token in a given collection of tweets in order for it to be included in the vocabulary.\n",
    "        \"\"\" \n",
    "        self.vocabulary = dict()\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\"\n",
    "         This function takes as input a numpy array containing a set of tweets and maps each unique word in the \n",
    "         tweets to its coresponding word ID as long as the frequency of the word within the tweets is greater than \n",
    "         or equal to the min_freq threshold and less than or equal to the max_freq threshold. The return value of\n",
    "         the function is the object itself.\n",
    "        \"\"\"\n",
    "        temp = [x.split(' ') for x in text]\n",
    "        tokens = sum(temp, [])\n",
    "        vocab = list(set(tokens))\n",
    "        i = 0\n",
    "        for token in vocab:\n",
    "            if self.min_df <= tokens.count(token) <= self.max_df:\n",
    "                self.vocabulary[token] = i\n",
    "                i = i + 1\n",
    "        return self\n",
    "                    \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "         This function takes a numpy array of tweets and returns a sparse matrix containing the feature vector\n",
    "         associated with each tweet. The mapping of the words present in each tweet and their corresponding \n",
    "         column ID in the sparse matrix is based on the dictionary created by the init() function. \n",
    "        \"\"\"    \n",
    "        data_matrix = []\n",
    "        for sent in text:\n",
    "            temp = [0.0]*len(self.vocabulary)\n",
    "            sent = sent.split(' ')\n",
    "            for word in sent:\n",
    "                if word in self.vocabulary.keys():\n",
    "                    id = self.vocabulary[word]\n",
    "                    temp[id] = temp[id] + 1\n",
    "            data_matrix.append(temp)\n",
    "        data_matrix = np.array(data_matrix)\n",
    "        return data_matrix\n",
    "                    \n",
    "    def getVocabulary(self):\n",
    "        \"\"\"\n",
    "         This function returns the dictionary object used to map each word to its corresponding word ID. \n",
    "        \"\"\"   \n",
    "        return self.vocabulary\n",
    "    \n",
    "    def displayVector(self, vec):\n",
    "        \"\"\"\n",
    "         This function takes as input a sparse vector and displays the list of words associated with the vector \n",
    "         along with their respective frequencies.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        for i in range(len(vec)):\n",
    "            if vec[i] >= 1:\n",
    "                k = list(self.vocabulary.keys())[list(self.vocabulary.values()).index(i)]\n",
    "                features.append((k,vec[i]))\n",
    "\n",
    "        print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Code for testing correctness of your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 2. 1. 3. 0. 1. 1.]]\n",
      "\n",
      "Display the feature vector (to check correctness of the word ID mapping)\n",
      "[('tweet', 1.0), ('is', 1.0), ('first', 1.0), ('my', 1.0), ('this', 1.0)]\n",
      "[('here', 1.0), ('second', 1.0), ('tweet', 1.0), ('is', 1.0), ('my', 1.0)]\n",
      "[('third', 1.0), ('tweet', 1.0), ('is', 1.0), ('my', 1.0), ('this', 1.0)]\n",
      "[('tweet', 2.0), ('is', 1.0), ('last', 3.0), ('my', 1.0), ('this', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "text = pd.Series(['this is my first tweet', 'here is my second tweet', 'this is my third tweet',\n",
    "                 'this tweet is my last last last tweet'])\n",
    "tokenizer = TwitterTokenizer().fit(text)\n",
    "X = tokenizer.transform(text)\n",
    "print(X)\n",
    "print('\\nDisplay the feature vector (to check correctness of the word ID mapping)')\n",
    "for i in range(text.shape[0]):\n",
    "    tokenizer.displayVector(X[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Apply the tokenizer to the twitter data and extract the predictor matrix (X) and target class (y) from the dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 6944) (50000,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TwitterTokenizer(min_df=5, max_df=1000).fit(data['text'].values)\n",
    "X = tokenizer.transform(data['text'].values)\n",
    "y = data['sentiment']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6944\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. TWITTER SENTIMENT CLASSIFICATION\n",
    "\n",
    "In this step, you need to write the code to implement two types of classifiers: (1) logistic regression and (2) a multi-layer neural network to classify the sentiment of the tweets. Your code must be implemented using only functions provided by the numpy and python standard library. You're not allowed to use `scikit-learn`, `pytorch`, `tensorflow`, `keras`, or other libraries to implement these steps.\n",
    "\n",
    "However, to ensure that the dataset you have created in part (c) is working correctly, you can test it first by running the code below (which uses the scikit-learn implementation for classification). Since both logistic regression and multi-layer neural network assume the classes are 0 or 1, you should convert the `positive` and `negative` class labels into 0 or 1 first before providing the training and test data to the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [[-0.79529474 -0.23973421 -0.233931   ... -0.27928087 -0.91436348\n",
      "   0.60642076]]\n",
      "Model intercept: [0.3208939]\n",
      "\n",
      "Model performance on training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.79      0.82     12439\n",
      "    positive       0.81      0.86      0.83     12561\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n",
      "\n",
      "Model performance on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.69      0.71     12459\n",
      "    positive       0.71      0.75      0.73     12541\n",
      "\n",
      "    accuracy                           0.72     25000\n",
      "   macro avg       0.72      0.72      0.72     25000\n",
      "weighted avg       0.72      0.72      0.72     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# Sample code to test correctedness of your dataset as input to a classifier\n",
    "##################################################################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,y_train)\n",
    "print('Model coefficients:', model.coef_)\n",
    "print('Model intercept:', model.intercept_)\n",
    "\n",
    "print('\\nModel performance on training set:')\n",
    "Ypred = model.predict(X_train)\n",
    "print(classification_report(y_train, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred = model.predict(X_test)\n",
    "print(classification_report(y_test, Ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data : 0.71916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of Test Data :', accuracy_score(y_test, Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:**  Write an implementation of the logistic regression classifier. There are 2 functions that must be implemented in this class: (1) a `fit` function to fit the classifier to the given input data using gradient descent algorithm, and (2) a `predict` function to apply the classifier to the test data. \n",
    "\n",
    "During training, the logistic regression classifier will minimize the following $\\ell_2$-regularized negative log-likehood function using the gradient descent algorithm:\n",
    "$$\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N \\bigg[ y_i \\log\\bigg(1 + e^{-\\mathbf{w}^T \\mathbf{x}_i - w_0}\\bigg )  + (1 - y_i) \\log\\bigg(1 + e^{\\mathbf{w}^T\\mathbf{x}_i + w_0}\\bigg) \\bigg] + C (\\|\\mathbf{w}\\|_2^2 + w_0^2)$$\n",
    "where $\\mathbf{w}$ is the vector of model coefficients, $w_0$ is the model intercept, $N$ is the size of the training data and $C$ is the regularization parameter. Both the model intercept and coefficients must be initialized to 0 and vector of zeros, respectively, before performing the gradient descent. You must compute the training loss at every iteration and store them in an array named loss. The training loss will be returned by the fit() function, which you can use to plot the convergence of the algorithm.\n",
    "\n",
    "Create a class named LogisticRegr for the logistic regression model. The model contains 2 functions:\n",
    "- `fit`: this function will estimate the parameters of the generalized linear model using the maximum likelihood approach with gradient descent algorithm. The pseudocode of the gradient descent algorithm is as follows:\n",
    "\n",
    "    - Initialize $\\mathbf{w}$ = [0,0,...,0]\n",
    "    - for iteration = 1 to maxiter do\n",
    "        - Update the weight: $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha\\nabla \\mathcal{L}$ where $\\alpha$ is the learning rate, $\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N [\\sigma(\\mathbf{x}_i) - y_i]\\mathbf{x}_i$ and $\\frac{\\partial \\mathcal{L}}{\\partial w_0} = \\frac{1}{N} \\sum_{i=1}^N [\\sigma(\\mathbf{x}_i) - y_i]$\n",
    "    - end\n",
    "\n",
    "The `fit` function must return a vector that contains the $\\ell_2$-norm of the prediction error, i.e., $\\|\\sigma(\\mathbf{X})-\\mathbf{y}\\|_2 = \\sqrt{\\sum_{i=1}^N [\\sigma(\\mathbf{x}_i) - y_i]^2}$ at every iteration.\n",
    "\n",
    "- `predict`: this function will compute the class conditional probability of each data point $\\mathbf{x}_i$ as $$P(y_{i} = 1 | \\mathbf{x}_i) \\equiv \\sigma(\\mathbf{x}_i) = \\frac{1}{1 + \\exp[-\\mathbf{w}^T\\mathbf{x}_i - w_0]}, \\ \\ \\ \\ \\ P(y_{i} = 0 | \\mathbf{x}_i) = 1 - \\sigma(\\mathbf{x}_i)$$\n",
    "The `predict` function will assign each data point to the class with larger probability, e.g., class 1 if $P(y_{i,pred} = 1 | \\mathbf{x}_i) > P(y_{i,pred} = 0 | \\mathbf{x}_i)$. The function should return both the predicted class as well as the class conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Fit the model to the training data. Set the `learning_rate` parameter to 0.05 and `maxiter` to 500. Print the regression coefficients and plot the error of the logistic regression model for each epoch. You will need to convert the class labels (positive or negative) to binary 0/1 values first before providing the data to the logistic regression classifier for training. **Note:** your model can be different from the results given by scikit-learn since the optimizer used in scikit-learn could be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegr():\n",
    "    \"\"\"\n",
    "    Implementation of logistic regression classifier.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "    def fit(self, X, y, maxiter = 100, learning_rate = 0.01, C=1):\n",
    "        \"\"\"\"\n",
    "            Input:  \n",
    "                X: N x d matrix of predictor attributes, where N is #training points and d is #predictor attributes\n",
    "                y: N x 1 binary vector of target attributes.\n",
    "                maxiter: maximum iteration before the gradient/subgradient descent algorithm terminates (default=100).\n",
    "                learning_rate: learning rate for gradient descent (default = 0.01).\n",
    "                C: Regularization penalty (default = 1).\n",
    "\n",
    "            Output: \n",
    "                error: maxiter x 1 vector containing the norm of the prediction error vector at each iteration\n",
    "                        (see description above).\n",
    "                        \n",
    "            The function will compute the model parameters and store them as variables: self.coef and self.intercept.\n",
    "            \n",
    "        \"\"\"   \n",
    "        N, D = X.shape\n",
    "        self.w = np.zeros(D).reshape(-1,1)\n",
    "        self.w0 = 0\n",
    "        error = []\n",
    "        for i in range(maxiter):\n",
    "            y_pred = self.sigmoid(X.dot(self.w) + self.w0).reshape(-1,1)\n",
    "            err = y_pred - y\n",
    "            grad_w = 1/N * (X.T.dot(err) + 2*C*self.w)\n",
    "            grad_w0 = 1/N * (np.sum(err) + 2*C*self.w0)\n",
    "            self.w -= learning_rate * grad_w\n",
    "            self.w0 -=learning_rate * grad_w0\n",
    "            error.append(np.linalg.norm(err))\n",
    "        \n",
    "        return error\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\"\n",
    "            Input:  \n",
    "                X: N x d matrix of predictor attributes, where N is #data points and d is #predictor attributes\n",
    "\n",
    "            Output: \n",
    "                Y_pred: N x 1 vector containing the predicted class of each data point (either 0 or 1)\n",
    "                Y_probs: N x 2 vector containing posterior probabilities of each data point in each of the 2 classes,\n",
    "                         where Y_probs[:,0] = P(y=0|x) and Y_probs[:,1] = P(y=1|x). \n",
    "        \"\"\"    \n",
    "\n",
    "        Y_probs = self.sigmoid(X.dot(self.w) + self.w0)\n",
    "        Y_pred = np.round(Y_probs)\n",
    "        \n",
    "        return (Y_pred, Y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [[ 1.52059359e-03]\n",
      " [ 5.01436728e-04]\n",
      " [-1.94735436e-03]\n",
      " ...\n",
      " [-1.66844412e-05]\n",
      " [ 1.98726185e-03]\n",
      " [-1.94970150e-03]]\n",
      "Model intercept: -0.02528374585951673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################\n",
    "# Code to convert the tweet classes into binary 0/1 labels\n",
    "########################################################################\n",
    "\n",
    "labels = {label: int(idx) for idx, label in enumerate(y.unique())}\n",
    "Y_train_bin = np.array([labels[label] for label in y_train.values]).reshape((-1,1))  \n",
    "Y_test_bin = np.array([labels[label] for label in y_test.values]).reshape((-1,1))  \n",
    "\n",
    "########################################################################\n",
    "# Code to train logistic regression model to the training data\n",
    "########################################################################\n",
    "\n",
    "model = LogisticRegr()\n",
    "error = model.fit(X_train, Y_train_bin, learning_rate = 0.05, maxiter=500)\n",
    "\n",
    "print('Model coefficients:', model.w)\n",
    "print('Model intercept:', model.w0 )\n",
    "\n",
    "\n",
    "plt.plot(error)\n",
    "plt.title('Logistic regression')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Apply the model to both training and test sets. Report the classification performance.\n",
    "\n",
    "**Note:** The numbers below are for illustrative purposes only. The actual results may vary depending on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on training set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.74     12561\n",
      "           1       0.77      0.56      0.65     12439\n",
      "\n",
      "    accuracy                           0.70     25000\n",
      "   macro avg       0.72      0.70      0.69     25000\n",
      "weighted avg       0.72      0.70      0.69     25000\n",
      "\n",
      "\n",
      "Model performance on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72     12541\n",
      "           1       0.75      0.54      0.63     12459\n",
      "\n",
      "    accuracy                           0.68     25000\n",
      "   macro avg       0.69      0.68      0.67     25000\n",
      "weighted avg       0.69      0.68      0.67     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model performance on training set:')\n",
    "\n",
    "Ypred_train = model.predict(X_train)\n",
    "\n",
    "print(classification_report(Y_train_bin, Ypred_train[0]))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "\n",
    "\n",
    "Ypred_test = model.predict(X_test)\n",
    "print(classification_report(Y_test_bin, Ypred_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data : 0.67868\n",
      "Accuracy of Train Data : 0.70012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of Test Data :', accuracy_score(Y_test_bin, Ypred_test[0]))\n",
    "print('Accuracy of Train Data :', accuracy_score(Y_train_bin, Ypred_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-layer Neural network\n",
    "\n",
    "This step requires you to implement a fully-connected neural network of arbitrary depth to solve the classification problem. For each layer you will implement a `forward` and a `backprop` function  separately. The `forward` function will receive inputs from previous layer, alongside with models parameters and will return an output for next layer and also store the data needed for the backward pass in a cache object. The `backprop` function receives the gradients from its next layer and use them to compute the gradients with respect to the inputs and weights. The last layer of the network is a fully connected layer with a sigmoid activation function. The network should be trained to minimize the cross entropy loss function. For all other hidden layers of the network, the user can specify the number of hidden nodes and type of activation function as hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7:** You will need to implement two classes: `Layer` and `MultiLayerNet`. The `Layer` class contains implementation for a single layer in the multi-layer network. The layer must store the following parameters:\n",
    "- `self.activation_type`: which takes either the value `ReLU` or `sigmoid`\n",
    "- `self.W`: this corresponds to the weight matrix W associated with the neural network\n",
    "- `self.b`: this corresponds to the bias parameter of the neural network\n",
    "\n",
    "The class also contains implementation of the `forward` and `backprop` functions that correspond to computations of the forward pass and backward pass of the neural network layer (see lecture notes for details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    \n",
    "    Implementation of a single layer in a multi-layer neural network. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, activation_type):\n",
    "        \"\"\"\"        \n",
    "            Input:  \n",
    "                input_dim: dimension of its input attributes\n",
    "                output_dim: dimension of its output\n",
    "                activation_type: 'ReLU' or 'sigmoid' (default = 'sigmoid')\n",
    " \n",
    "            This function should store the activation_type and creates two variables:\n",
    "            - self.W: a matrix of size output_dim x input_dim to represent the weight parameter of the neural network\n",
    "            - self.b: a vector of length output_dim to represent the bias parameter of the neural network\n",
    "            \n",
    "            You should initialize self.W randomly from a Gaussian distribution with mean 0 and standard deviation (scale) \n",
    "            equals to 0.1\n",
    "            \n",
    "        \"\"\"    \n",
    "\n",
    "        self.input_size = input_dim\n",
    "        self.output_size = output_dim\n",
    "        self.activation_type = activation_type\n",
    "\n",
    "        self.W = np.random.normal(0, 0.1,(self.input_size, self.output_size))\n",
    "        self.b = np.random.random((1, self.output_size))\n",
    "\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self,z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def ReLU(self,z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def ReLU_derivative(self,z):\n",
    "        return np.where(z <= 0, 0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            This function computes the forward propagation step of a layer in the neural network. \n",
    "            \n",
    "            Input:  \n",
    "                x: a vector whose length should be equal to input_dim\n",
    "\n",
    "            Output:\n",
    "                a: output activation.   \n",
    "\n",
    "            The output activation for a given input vector x is computed as follows: sigma[Wx + b], \n",
    "            where sigma is the activation function. \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.z = np.dot(x, self.W) + self.b\n",
    "\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            self.a = self.sigmoid(self.z)\n",
    "        elif self.activation_type == 'ReLU':\n",
    "            self.a = self.ReLU(self.z)\n",
    "        else:\n",
    "            raise NameError('activation type is not defined!')   \n",
    "    \n",
    "\n",
    "        return self.a\n",
    "    \n",
    "    def backprop(self, x, delta_l, next_layer=None):\n",
    "        \"\"\"\n",
    "        This function performs the backward propagation step of the neural network. Specifically,\n",
    "        it calculates the gradient of the loss function with respect to the weight and bias parameters\n",
    "        and stores them as self.grad_W and self.grad_b, respectively.\n",
    "\n",
    "        Input:\n",
    "            x: input vector.\n",
    "            delta_l: loss propagated from its next layer.\n",
    "            next_layer: next layer object, which is needed to update delta_l.\n",
    "\n",
    "        Output:\n",
    "            delta_l_minus_1: gradient to be transmitted to its previous layer\n",
    "\n",
    "        Note:\n",
    "        1. If next_layer=None, this implies it is the last layer of the multi-layer neural network.\n",
    "        2. The formula for computing the gradient delta_l depends on the activation function (RELU or sigmoid).\n",
    "        3. The update formula for self.W and self.b also depends on the activation function (RELU or sigmoid).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            derivative = self.sigmoid_derivative(self.a)\n",
    "        elif self.activation_type == 'ReLU':\n",
    "            derivative = self.ReLU_derivative(self.z)\n",
    "        else:\n",
    "            raise NameError('activation type is not defined!')\n",
    "\n",
    "        gradient = delta_l * derivative\n",
    "\n",
    "        self.grad_W = np.dot(x.T, gradient)\n",
    "        self.grad_b = np.sum(gradient, axis=0, keepdims=True)\n",
    "\n",
    "        if next_layer is not None:\n",
    "            delta_l_minus_1 = np.dot(gradient, next_layer.W.T) * derivative\n",
    "        else:\n",
    "            delta_l_minus_1 = delta_l\n",
    "            \n",
    "        return delta_l_minus_1\n",
    "\n",
    "\n",
    "\n",
    "    def backprop(self, x, delta_l, next_layer = None):\n",
    "       \n",
    "        \"\"\"   This function performs the backward propagation step of the neural network. Specifically,\n",
    "           it calculates the gradient of the loss function with respect to the weight and bias parameters\n",
    "           and stores them as self.grad_W and self.grad_b, respectively.\n",
    "            \n",
    "            Input:  \n",
    "                x: input vector.\n",
    "                delta_l: loss propagated from its next layer.\n",
    "                next_layer: next layer object, which is needed to update delta_l.\n",
    "\n",
    "            Output:\n",
    "                delta_l: gradient to be transmitted to its previous layer\n",
    "                \n",
    "            Note:\n",
    "            1. If next_layer=None, this implies it is the last layer of the multi-layer neural network. \n",
    "            2. The formula for computing the gradient delta_l depends on the activation function (RELU or sigmoid).\n",
    "            3. The update formula for self.W and self.b also depends on the activation function (RELU or sigmoid).\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        if self.activation_type =='sigmoid':\n",
    "            gradient = delta_l * self.sigmoid_derivative(self.a)\n",
    "\n",
    "        elif self.activation_type == 'ReLU':\n",
    "            gradient = delta_l * self.ReLU_derivative(self.a)\n",
    "        else:\n",
    "            raise NameError('activation type is not defined!') \n",
    "        \n",
    "        if self.activation_type == 'sigmoid':\n",
    "            derivative = self.sigmoid_derivative(self.a)\n",
    "        elif self.activation_type == 'ReLU':\n",
    "            derivative = self.ReLU_derivative(self.a)\n",
    "        else:\n",
    "            raise NameError('activation type is not defined!')\n",
    "\n",
    "        self.grad_W = np.dot(x.T, gradient)\n",
    "        self.grad_b = np.sum(gradient, axis=0, keepdims=True)\n",
    "\n",
    "        if next_layer is None:\n",
    "            delta_l_minus_1 = np.dot(gradient, self.W.T)\n",
    "\n",
    "        elif next_layer is not None:\n",
    "            delta_l_minus_1 = np.dot(gradient,next_layer.W)\n",
    "        \n",
    "    \n",
    "        return delta_l_minus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNet(object):\n",
    "    \"\"\"\n",
    "   \n",
    "       Implementation of a multi-layer neural network.\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers, learning_rate = 0.001, epochs = 50):\n",
    "        \"\"\"\"        \n",
    "            Input:  \n",
    "                layers: a list of layers associated with the multi-layer neural network.\n",
    "                learning_rate: learning rate for gradient descent algorithm.\n",
    "                epochs: maximum number of epochs to train the neural network.\n",
    "\n",
    "            This function should store the input variables locally as self.layers, self.learning_rate,\n",
    "            and self.epochs. These variables are needed when the network performs forward and back propagation. \n",
    "            \n",
    "        \"\"\"    \n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs =  epochs\n",
    "        self.layers = layers\n",
    "\n",
    "    \n",
    "    def cross_entropy_loss_derivative(self,y_hat, y):\n",
    "        return (y_hat - y) / ((y_hat * (1 - y_hat))) \n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            This function computes the forward propagation step of the multi-layer neural network. \n",
    "            Given an input vector x, it will first apply the forward function from the first layer,\n",
    "            then passes its output activation as input to the next layer, and so on, until it \n",
    "            receives the output activation from the last layer of the network. \n",
    "            \n",
    "            Input:  \n",
    "                x: an input vector of predictor attribute values.\n",
    "\n",
    "            Output:\n",
    "                y_pred: predicted value given by the output activation of the last layer of the network. \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        y_pred = x\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "            This function implements the backward propagation step of the multi-layer neural network. \n",
    "            Starting from the last layer, it computes the gradient of the loss function and back-propagates \n",
    "            the gradient to its previous layer and repeat the process until the gradient of the first\n",
    "            layer is computed. \n",
    "            \n",
    "            Input:  \n",
    "                x: input vector.\n",
    "                y: ground truth class.\n",
    "                \n",
    "        \"\"\"\n",
    "        y_hat = self.forward(x)\n",
    "        delta_l = self.cross_entropy_loss_derivative(y_hat, y)\n",
    "\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            if i == len(self.layers) - 1:\n",
    "                delta_l_minus_1 = self.layers[i].backprop(self.layers[i - 1].a, delta_l)\n",
    "            elif i == 0:\n",
    "                delta_l_minus_1 = self.layers[i].backprop(x, delta_l, self.layers[i + 1])\n",
    "            else:\n",
    "                delta_l_minus_1 = self.layers[i].backprop(self.layers[i - 1].a, delta_l, self.layers[i + 1])\n",
    "\n",
    "            delta_l = delta_l_minus_1\n",
    "\n",
    "        return delta_l\n",
    "        \n",
    "\n",
    "   \n",
    "    def loss(self, y_hat, y):\n",
    "        \"\"\"\n",
    "            This function computes the cross-entropy loss.\n",
    "            \n",
    "            Input: \n",
    "                y: vector of ground truth classes.\n",
    "                y_hat: vector of predicted classes. \n",
    "        \n",
    "            Output:\n",
    "                Cross entropy loss.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)) * (-1/len(y))\n",
    "        \n",
    "        return loss \n",
    "\n",
    "\n",
    "    def fit(self, X, Y, plot_loss = False):\n",
    "        \"\"\"\n",
    "            This function is invoked when training the neural network model.\n",
    "        \n",
    "            Input:\n",
    "                X: A 2-d numpy array consisting of the predictor attribute values of the training data  \n",
    "                Y: A numpy array vector consisting of the target class labels of the training data\n",
    "                plot_loss: A True/False value. It will display the average loss at each epoch average if set to True. \n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            epoch_loss =0\n",
    "            for x, y in zip(X, Y):\n",
    "                \n",
    "                x = np.array([x])  \n",
    "                y = np.array([y])  \n",
    "                y_hat = self.forward(x) \n",
    "                self.backprop(x, y)\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    layer.W -= self.learning_rate * layer.grad_W\n",
    "                    layer.b -= self.learning_rate * layer.grad_b\n",
    "\n",
    "                loss = self.loss(y_hat, y)\n",
    "                epoch_loss += loss\n",
    "\n",
    "            epoch_loss /= len(X)\n",
    "            loss_history.append(epoch_loss)\n",
    "       \n",
    "        if plot_loss:\n",
    "            plt.plot(loss_history)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('loss')                   \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            This function is used to predict class label of any input data X. Specifically,\n",
    "            for each data point, it will call the forward() function to calculate the\n",
    "            output activation of the last layer of the neural network. It will then predict\n",
    "            the class label based on the value of the output activation. If the output is\n",
    "            greater than or equal to 0.5, it will predict the class label to be 1. Otherwise, \n",
    "            it will predict the class label to be 0.\n",
    "\n",
    "            Input:\n",
    "                X: A 2-dimensional numpy array of predictor attribute values. \n",
    "\n",
    "            Output:\n",
    "                Y_hat: A numpy array that contains the predicted class label of the input data.\n",
    "                       Each element of the vector is either 0 or 1.\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        Y_hat = self.forward(X)\n",
    "        return np.round(Y_hat)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Create a 3-layer network and train the neural network to classify the tweets. The architecture of the 3 layer network is as follows:\n",
    "\n",
    "    - Hidden layer 1: #hidden nodes = 10, activation function = sigmoid\n",
    "    - Hidden layer 2: #hidden nodes = 5, activation function = ReLU\n",
    "    - Output layer: #output nodes = 1, activation function = sigmoid\n",
    "    \n",
    "Set the learning rate to 0.001 and maximum epoch to be 100. You can also try other configuration and look for the best possible classification result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [21:39<00:00, 12.99s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################\n",
    "# Code to convert the classes into binary 0/1 labels\n",
    "########################################################################\n",
    "\n",
    "labels = {label: int(idx) for idx, label in enumerate(y.unique())}\n",
    "Y_train_bin = np.array([labels[label] for label in y_train.values]).reshape((-1,1))  \n",
    "Y_test_bin = np.array([labels[label] for label in y_test.values]).reshape((-1,1))  \n",
    "\n",
    "########################################################################\n",
    "# Code to train the multi-layer network on the twitter data\n",
    "########################################################################\n",
    "\n",
    "layers = [Layer(X_train.shape[1],10,'sigmoid'),Layer(10,5,'ReLU'),Layer(5,1,'sigmoid') ]\n",
    "model = MultiLayerNet(layers, learning_rate= 0.001, epochs =100)\n",
    "model.fit(X_train, Y_train_bin, plot_loss = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Apply the model to the training and test data and report their performance.\n",
    "\n",
    "**Note:** The numbers below are for illustrative purposes only. The actual results may vary depending on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on training set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashashvini/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yashashvini/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yashashvini/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     12561\n",
      "           1       0.00      0.00      0.00     12439\n",
      "\n",
      "    accuracy                           0.50     25000\n",
      "   macro avg       0.25      0.50      0.33     25000\n",
      "weighted avg       0.25      0.50      0.34     25000\n",
      "\n",
      "\n",
      "Model performance on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67     12541\n",
      "           1       0.00      0.00      0.00     12459\n",
      "\n",
      "    accuracy                           0.50     25000\n",
      "   macro avg       0.25      0.50      0.33     25000\n",
      "weighted avg       0.25      0.50      0.34     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashashvini/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yashashvini/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/yashashvini/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Model performance on training set:')\n",
    "Ypred = model.predict(X_train)\n",
    "\n",
    "\n",
    "print(classification_report(Y_train_bin, Ypred))\n",
    "\n",
    "print('\\nModel performance on test set:')\n",
    "Ypred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(Y_test_bin, Ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
